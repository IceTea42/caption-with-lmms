{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb79555-4901-4141-ab62-722131b9851f",
   "metadata": {},
   "source": [
    "# Caption Images with Large MultiModal Models\n",
    "\n",
    "This notebook lets you caption your images using different Large MultiModal Models.\n",
    "\n",
    "Once equipped with the packages precised in the requirements file, execute all the cells to see a user intarface where you can insert your own images and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8718ee5-4b2d-4cd3-883f-56481da56641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 20 17:08:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P8              10W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b440e1-1d60-4817-b2f7-01007df04d35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /opt/conda\n",
      "deepseek              *  /opt/conda/envs/deepseek\n",
      "jupyterlab               /opt/conda/envs/jupyterlab\n",
      "moondream                /opt/conda/envs/moondream\n",
      "pytorch                  /opt/conda/envs/pytorch\n",
      "tensorflow               /opt/conda/envs/tensorflow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4fab8b-e7e7-4420-a820-80cd508dc5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/deepseek/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7236101610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "import gradio as gr\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "from transformers import AutoModelForCausalLM, CodeGenTokenizerFast\n",
    "from PIL import Image\n",
    "\n",
    "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
    "from deepseek_vl.utils.io import load_pil_images\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863f7cca-a336-48a6-9431-4d8cad8d6096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_boorus_spacy(description, nlp):\n",
    "    \"\"\"Use traditional NLP to convert caption to tags.\"\"\"\n",
    "    doc = nlp(description)\n",
    "    notions_detailed = []\n",
    "    notions_base = []\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # Detailed notions, keep adjectives\n",
    "        detailed_phrase_tokens = [token.text for token in chunk if\n",
    "                                  not token.is_stop or token.pos_ in ['ADJ', 'NOUN', 'PROPN']]\n",
    "        detailed_phrase = ' '.join(detailed_phrase_tokens)\n",
    "        if detailed_phrase:  \n",
    "            notions_detailed.append(detailed_phrase)\n",
    "        # Base notions\n",
    "        base_phrase_tokens = [token.text for token in chunk if token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop]\n",
    "        base_phrase = ' '.join(base_phrase_tokens)\n",
    "        if base_phrase: \n",
    "            notions_base.append(base_phrase)\n",
    "\n",
    "    notions_detailed_str = ', '.join(set(notions_detailed))\n",
    "    notions_base_str = ', '.join(set(notions_base))\n",
    "    # print(\"base:\", notions_base_str)\n",
    "    # print(\"detailed:\", notions_detailed_str)\n",
    "    return notions_detailed_str, notions_base_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631cf75e-151d-4c8f-9a71-ab581f6bd7ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def caption_image(image_path, model_name, prompt, output_type, nlp):\n",
    "    \"\"\"Caption each image in the list and save the results in a file.\"\"\"\n",
    "\n",
    "    if \"DeepSeek\" == model_name:\n",
    "        # Load the model\n",
    "        ds_model_id = (\"deepseek-ai/deepseek-vl-1.3b-chat\")\n",
    "        ds_vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(ds_model_id)\n",
    "        ds_vl_tokenizer = ds_vl_chat_processor.tokenizer\n",
    "        ds_vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(ds_model_id, trust_remote_code=True)\n",
    "        ds_vl_gpt = ds_vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "        print(\"Loaded DeepSeek weights and set inference mode.\")\n",
    "        # Embed input image\n",
    "        conversation = [{\"role\": \"User\",\n",
    "                         \"content\": \"<image_placeholder>\" + prompt,\n",
    "                         \"images\": [image_path]},\n",
    "                        {\"role\": \"Assistant\", \"content\": \"\"}]       \n",
    "        pil_images = load_pil_images(conversation)\n",
    "        prepare_inputs = ds_vl_chat_processor(conversations=conversation,\n",
    "            images=pil_images, force_batchify=True).to(ds_vl_gpt.device)        \n",
    "        inputs_embeds = ds_vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "        # Infer      \n",
    "        start_time = time.time()\n",
    "        outputs = ds_vl_gpt.language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=prepare_inputs.attention_mask,\n",
    "            pad_token_id=ds_vl_tokenizer.eos_token_id,\n",
    "            bos_token_id=ds_vl_tokenizer.bos_token_id,\n",
    "            eos_token_id=ds_vl_tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "        answer = ds_vl_tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "        duration = time.time() - start_time\n",
    "        info = f\"Inference time: {duration:.3f} seconds\"\n",
    "    elif \"MoonDream\" in model_name:\n",
    "        # Load the model\n",
    "        md_model_id = \"vikhyatk/moondream2\"\n",
    "        md_revision = \"2024-03-06\"\n",
    "        md_model: AutoModelForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "            md_model_id, trust_remote_code=True, revision=md_revision)\n",
    "        md_tokenizer: CodeGenTokenizerFast = CodeGenTokenizerFast.from_pretrained(md_model_id)\n",
    "        print(\"Loaded MoonDream weights.\")\n",
    "        # Embed input image\n",
    "        image = Image.open(image_path)\n",
    "        enc_image = md_model.encode_image(image)\n",
    "        # Infer\n",
    "        start_time = time.time()\n",
    "        answer = md_model.answer_question(enc_image, prompt, md_tokenizer)\n",
    "        duration = time.time() - start_time\n",
    "        info = f\"Inference time: {duration:.3f} seconds\"\n",
    "    else:\n",
    "        print(\"Error:\", model_name, \"is not an available LVM.\")       \n",
    "    print(model_name, \":\", answer)\n",
    "    \n",
    "    # Get the tag version\n",
    "    if \"tag\" == output_type:\n",
    "        answer = answer.replace(\" and\", \",\").replace(\",,\", \",\")\n",
    "        detailed, base = get_boorus_spacy(answer, spacy.load(\"en_core_web_sm\"))\n",
    "        answer = base if \"base\" in output_type else detailed\n",
    "        \n",
    "    return answer, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340ebc45-7647-4b9b-8b67-6f1864ffeefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"DeepSeek\", \"MoonDream\"]\n",
    "output_types = [\"tag\", \"caption\"]\n",
    "\n",
    "\n",
    "def demo_caption_image(im_in, model_name=\"DeepSeek\", output_type=\"tag\", \n",
    "                      prompt=\"Analyze this image, highlighting the most significant components and their meanings.\"):\n",
    "    \"\"\"Wrap image captioning function to hide some arguments from the user.\"\"\"\n",
    "    cache_dir = os.path.join(\".\", \"temp\")\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    im_in_path = os.path.join(cache_dir, time.strftime(\"%Y%m%d_%H%M%S\") + \".png\")\n",
    "    print(\"im_in_path:\", im_in_path)\n",
    "    pil_im = Image.fromarray(im_in)\n",
    "    pil_im.save(im_in_path)\n",
    "    # if \"DeepSeek\" == model_name: \n",
    "    #     model = ds_vl_gpt\n",
    "    #     tokenizer = ds_vl_tokenizer\n",
    "    # elif \"MoonDream\" == model_name:\n",
    "    #     model = md_model\n",
    "    #     tokenizer = md_tokenizer\n",
    "    # else:\n",
    "    #     info = \"Error: This model has not been added.\"\n",
    "    #     print(info)\n",
    "    caption, info = caption_image(image_path=im_in_path, model_name=model_name, # model=vl_gpt, tokenizer=tokenizer, \n",
    "                            prompt=prompt, output_type=output_type, nlp=spacy.load(\"en_core_web_sm\"))\n",
    "    return caption, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3dada8f-c0e6-45f7-9933-4ce496c65327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://209c1719ed0cdd7189.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://209c1719ed0cdd7189.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im_in_path: ./temp/20240320_170946.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MoonDream weights.\n",
      "MoonDream : A woman is positioned in the center of the image, wearing a hat. The background features a wall, and the image appears to have been taken in a dimly lit environment.\n",
      "im_in_path: ./temp/20240320_171127.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MoonDream weights.\n",
      "MoonDream : A woman is positioned in the center of the image, wearing a hat. The background features a wall, and the image appears to have been taken in a dimly lit environment.\n",
      "im_in_path: ./temp/20240320_171305.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DeepSeek weights and set inference mode.\n",
      "DeepSeek : The image is a close-up portrait of a woman with a profile view. She has long, dark hair that cascades down her shoulders. The woman is wearing a large, wide-brimmed hat with a beige or light brown color and a feathery, purple-colored trim. The hat appears to be of a vintage style, possibly reminiscent of the 1920s or 1930s. The background is blurred, but it suggests an indoor setting with warm lighting, possibly a room with wooden beams or a similar architectural feature. The woman's expression is neutral, and she is looking directly at the camera. There are no visible texts or discernible brands in the image. The style of the image is reminiscent of fashion photography, focusing on the woman's attire and the composition of the background.\n",
      "im_in_path: ./temp/20240320_171346.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DeepSeek weights and set inference mode.\n",
      "DeepSeek : The image is a close-up portrait of a woman with a profile view. She has long, dark hair that cascades down her shoulders. The woman is wearing a large, wide-brimmed hat with a beige or light brown color and a feathery, purple-colored trim. The hat appears to be of a vintage style, possibly reminiscent of the 1920s or 1930s. The background is blurred, but it suggests an indoor setting with warm lighting, possibly a room with wooden beams or a similar architectural feature. The woman's expression is neutral, and she is looking directly at the camera. There are no visible texts or discernible brands in the image. The style of the image is reminiscent of fashion photography, focusing on the woman's attire and the composition of the background.\n",
      "im_in_path: ./temp/20240320_174847.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DeepSeek weights and set inference mode.\n",
      "DeepSeek : The image is a close-up portrait of a woman with a profile view. She has long, dark hair that cascades down her shoulders. The woman is wearing a large, wide-brimmed hat with a beige or light brown color and a feathery, purple-colored trim. The hat appears to be of a vintage style, possibly reminiscent of the 1920s or 1930s. The background is blurred, but it suggests an indoor setting with warm lighting, possibly a room with wooden beams or a similar architectural feature. The woman's expression is neutral, and she is looking directly at the camera. There are no visible texts or discernible brands in the image. The style of the image is reminiscent of fashion photography, focusing on the woman's attire and the composition of the background.\n",
      "im_in_path: ./temp/20240320_174930.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DeepSeek weights and set inference mode.\n",
      "DeepSeek : The image is a close-up portrait of a woman with a profile view. She has long, dark hair that cascades down her shoulders. The woman is wearing a large, wide-brimmed hat with a beige or light brown color and a feathery, purple-colored trim. The hat appears to be of a vintage style, possibly reminiscent of the 1920s or 1930s. The background is blurred, but it suggests an indoor setting with warm lighting, possibly a room with wooden beams or a similar architectural feature. The woman's expression is neutral, and she is looking directly at the camera. There are no visible texts or discernible brands in the image. The style of the image is reminiscent of fashion photography, focusing on the woman's attire and the composition of the background.\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=demo_caption_image,\n",
    "    inputs=[\n",
    "        gr.Image(label=\"Input Image\"),        \n",
    "        gr.Dropdown(model_names, label=\"Choose a LVM\", value=\"DeepSeek\"),\n",
    "        gr.Dropdown(output_types, label=\"Choose output type\", value=\"caption\"),\n",
    "        gr.Textbox(label=\"Captioning Prompt (optional)\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Result\"),\n",
    "        gr.Textbox(label=\"Information\"),\n",
    "    ],\n",
    "    allow_flagging=\"never\",\n",
    "    title=\"Image Captioning with Large Vision Models\",\n",
    "    description=\"Upload an image, pick a LVM and a captioning style.\"\n",
    "    \"\\n\\nThe generated captions and the runtime expressed in seconds will be shown.\"\n",
    "    # \"\\n\\nNote: If there is an error, your image might be too big for the GPU.\",\n",
    ")\n",
    "demo.launch(share=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9405ad-5c02-44c6-98be-98aa376c2bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "deepseek",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "deepseek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
